{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118292a5",
   "metadata": {},
   "source": [
    "## 1단계: Activation Vector 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9becfd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/socar_open_set'\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'weights')\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test')\n",
    "REJECT_PATH = os.path.join(DATA_PATH, 'reject')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # 여기서 'cuda'가 출력되어야 GPU와 연결이 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e24ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 파이프라인에는 이전에 사용했던 create_dataloader를 그대로 사용\n",
    "def create_dataloader(path, batch_size, istrain):\n",
    "    nearest_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "    normalize = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((320,320), interpolation=nearest_mode),\n",
    "        torchvision.transforms.CenterCrop((224,224)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.ColorJitter(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    test_transformer = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((320,320), interpolation=nearest_mode),\n",
    "        torchvision.transforms.CenterCrop((224,224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    if istrain:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=train_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    else:\n",
    "        data = torchvision.datasets.ImageFolder(path, transform=test_transformer)\n",
    "        dataloader = torch.utils.data.DataLoader(data, shuffle=False)\n",
    "\n",
    "    return dataloader, data\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b45e311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded the Network Weight!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 데이터를 연결하고, 미리 준비해 둔 모델을 불러오기\n",
    "# 클래스가 atower_b5, balsan_b5, balsan_b6, dcube_b6로 총 4개인 모델\n",
    "train_loader, _train_data = create_dataloader(TRAIN_PATH, 1, False)\n",
    "target_class_num = len(os.listdir(TRAIN_PATH))\n",
    "\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(\n",
    "    net.fc.in_features,\n",
    "    target_class_num\n",
    ")\n",
    "\n",
    "saved_weight_path = os.path.join(MODEL_PATH, 'classifier_acc_0.96008.pth')\n",
    "net.load_state_dict(torch.load(saved_weight_path, map_location=device))\n",
    "print('Successfully Loaded the Network Weight!')\n",
    "net.eval()\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2af62",
   "metadata": {},
   "source": [
    "* OpenMax에 필요한 데이터는 분류에 성공한 데이터의 Activation Vector\n",
    "* Softmax 층에 입력되는 값이 Activation Vector이니 torch.softmax()의 입력이 되는 값에서 뽑아오면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a877f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = list()\n",
    "train_actvecs = list()\n",
    "train_outputs_softmax = list()\n",
    "train_labels = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for idx, (img, label) in enumerate(train_loader):\n",
    "      img = img.to(device)\n",
    "      label = label.to(device)\n",
    "\n",
    "      out = net(img)\n",
    "      out_actvec = out.cpu().detach().numpy()[0]\n",
    "      out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
    "      out_pred = int(torch.argmax(out).cpu().detach().numpy())\n",
    "      out_label = int(label.cpu().detach().numpy())\n",
    "\n",
    "      train_actvecs.append(out_actvec) # component 1: softmax 전의 Activation Vector\n",
    "      train_preds.append(out_pred) # componenet 2: 각 데이터에 대한 예측값\n",
    "      train_outputs_softmax.append(out_softmax) # component 3: 각 데이터에 대한 softmax 확률\n",
    "      train_labels.append(out_label) # component 4: 각 데이터에 대한 Label (정답)\n",
    "\n",
    "train_actvecs = np.asarray(train_actvecs)\n",
    "train_preds = np.asarray(train_preds)\n",
    "train_outputs_softmax = np.asarray(train_outputs_softmax)\n",
    "train_labels = np.asarray(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821e7ce",
   "metadata": {},
   "source": [
    "* 모델에서 나온 Activation Vector를 모두 사용하지 않는다는 점\n",
    "* OpenMax에서는 모델이 정답을 맞힌 Activation Vector만 사용\n",
    "* 올바른 경우의 Activation Vector만 사용하겠다는 이야기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3572bd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation vector:  (4790, 4)\n",
      "Labels:  (4790,)\n"
     ]
    }
   ],
   "source": [
    "train_correct_actvecs = train_actvecs[train_labels==train_preds]\n",
    "train_correct_labels = train_labels[train_labels==train_preds]\n",
    "print('Activation vector: ', train_correct_actvecs.shape)\n",
    "print('Labels: ', train_correct_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53febd47",
   "metadata": {},
   "source": [
    "## 2단계: Weibull-Distribution\n",
    "* 앞서 추출해 낸 Activation Vector는 아래처럼 사용됨\n",
    "* 1.Activation Vector를 클래스마다 나눠 담습니다.\n",
    "* 2.클래스별로 나눠진 Activation Vector별 평균으로부터 가장 먼 100개의 Vector를 이용해 베이불 분포의 모수를 추출합니다.\n",
    "* 3.각 클래스당 베이불 분포의 모수들을 저장해 둡니다.\n",
    "* 우리가 사용할 베이불 분포의 모수는 shape, loc, scale로 3개이고, 클래스는 4개이므로 총 12개의 숫자가 있음\n",
    "* 아직 클래스가 5개가 되지 않았다는 점에 유의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8b0082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_idx:  0\n",
      "(1250, 4)\n",
      "class_idx:  1\n",
      "(1170, 4)\n",
      "class_idx:  2\n",
      "(1179, 4)\n",
      "class_idx:  3\n",
      "(1191, 4)\n"
     ]
    }
   ],
   "source": [
    "# 베이불 분포의 모수를 얻기\n",
    "class_means = list()\n",
    "dist_to_means = list()\n",
    "mr_models = {}\n",
    "\n",
    "for class_idx in np.unique(train_labels):\n",
    "    \n",
    "    print('class_idx: ', class_idx)\n",
    "    class_act_vec = train_correct_actvecs[train_correct_labels==class_idx]\n",
    "    print(class_act_vec.shape)\n",
    "    \n",
    "    class_mean = class_act_vec.mean(axis=0)\n",
    "    class_means.append(class_mean)\n",
    "    \n",
    "    dist_to_mean = np.square(class_act_vec - class_mean).sum(axis=1) # 각 activation vector의 거리를 계산\n",
    "    dist_to_mean_sorted = np.sort(dist_to_mean).astype(np.float64) # 거리를 기준으로 오름차순 정렬\n",
    "    dist_to_means.append(dist_to_mean_sorted)\n",
    "\n",
    "    shape, loc, scale = stats.weibull_max.fit(dist_to_mean_sorted[-100:]) # 거리가 가장 먼 100개를 사용하여 모수 추출\n",
    "    \n",
    "    mr_models[str(class_idx)] = {\n",
    "        'shape':shape,\n",
    "        'loc':loc,\n",
    "        'scale':scale\n",
    "    }\n",
    "    \n",
    "class_means = np.asarray(class_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738bfef",
   "metadata": {},
   "source": [
    "↑\n",
    "* 이렇게 얻은 모수는 shape, loc, scale\n",
    "* shape은 베이불 분포의 모양을 결정\n",
    "<img src=\"https://i.ibb.co/5WDMjV3/1109-1.jpg\">\n",
    "* loc은 분포의 가로축 평행 이동을 뜻하고, scale은 분포가 얼마나 넓게 퍼져있는지는 뜻함\n",
    "<img src=\"https://i.ibb.co/GckMK4p/1109-3.jpg\">\n",
    "* 정상적으로 분류된 Activation Vector를 이용해 베이불 모수를 얻었다는 것은 극단값을 판별할 기준을 얻었다는 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3a47ec",
   "metadata": {},
   "source": [
    "## OpenMax 적용\n",
    "* 앞서 계산된 모수는 극단값을 판별할 기준\n",
    "* 모든 클래스에서 평범하지 않다고 판단된다면 어떤 클래스에도 속하지 않는다는 이야기이니 reject라는 클래스로 분류\n",
    "* OpenMax 확률을 계산하는 함수를 만들어 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d22409",
   "metadata": {},
   "source": [
    "##### OpenMax 확률\n",
    "* 지금까지 계산한 것은 베이불 분포의 모수 뿐\n",
    "* 이미지와 베이불 분포의 모수를 이용해서 최종 분류를 할 수 있는 기준을 만들기\n",
    "* 우선 베이불 분포로부터 이미지의 확률을 계산\n",
    "* 베이불 분포 확률이 높다는 이야기는 평범하지 않다고 판별하는 것 즉, 확률이 낮을수록 평범하다는 이야기\n",
    "* weight×ActivationVector\n",
    "* 각 이미지의 확률(score)을 뺀 값을 가중치(weight)\n",
    "* 이 가중치를 Activation Vector에 곱해줌\n",
    "* 만약 각 이미지가 정해진 클래스에 속할수록 이 값은 높은 값이 나옴\n",
    "* 반대로 어떤 클래스에 속하지 않을수록 낮은 값이 나옴\n",
    "* 마지막에 reject class를 추가하여 새로운 값을 추가\n",
    "* ∑(1−weight)(ActivationVector)\n",
    "* 위 식을 계산하면 reject class에 속할수록 높은 값이 나옴\n",
    "* 만약 모든 클래스에 포함될 확률이 낮다면 이 값만 높게 나옴\n",
    "* 이걸 reject확률로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ae3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reject 클래스 추가\n",
    "def compute_openmax(actvec, class_means, mr_models):\n",
    "    dist_to_mean = np.square(actvec - class_means).sum(axis=1)\n",
    "\n",
    "    scores = list()\n",
    "    for class_idx in range(len(class_means)):\n",
    "        params = mr_models[str(class_idx)]\n",
    "        score = stats.weibull_max.cdf(\n",
    "            dist_to_mean[class_idx],\n",
    "            params['shape'],\n",
    "            params['loc'],\n",
    "            params['scale']\n",
    "        )\n",
    "        scores.append(score)\n",
    "    scores = np.asarray(scores)\n",
    "    \n",
    "    weight_on_actvec = 1 - scores # 각 class별 가중치\n",
    "    rev_actvec = np.concatenate([\n",
    "        weight_on_actvec * actvec, # known class에 대한 가중치 곱\n",
    "        [((1-weight_on_actvec) * actvec).sum()] # unknown class에 새로운 계산식\n",
    "    ])\n",
    "    \n",
    "    openmax_prob = np.exp(rev_actvec) / np.exp(rev_actvec).sum()\n",
    "    return openmax_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134119bc",
   "metadata": {},
   "source": [
    "### Inference 함수\n",
    "* threshold 기법\n",
    "* 계산한 최대 확률이 모두 다 낮은 경우라면 강제로 reject클래스로 분류해주는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1d3c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(actvec, threshold, target_class_num, class_means, mr_models):\n",
    "    openmax_prob = compute_openmax(actvec, class_means, mr_models)\n",
    "    openmax_softmax = np.exp(openmax_prob)/sum(np.exp(openmax_prob))\n",
    "\n",
    "    pred = np.argmax(openmax_softmax)\n",
    "    if np.max(openmax_softmax) < threshold:\n",
    "        pred = target_class_num\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb420a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold 탐색을 쉽게 하기 위해 함수\n",
    "def inference_dataloader(net, data_loader, threshold, target_class_num, class_means, mr_models, is_reject=False):\n",
    "    result_preds = list()\n",
    "    result_labels = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for idx, (img, label) in enumerate(data_loader):\n",
    "          img = img.to(device)\n",
    "          label = label.to(device)\n",
    "\n",
    "          out = net(img)\n",
    "          out_actvec = out.cpu().detach().numpy()[0]\n",
    "          out_softmax = torch.softmax(out, 1).cpu().detach().numpy()[0]\n",
    "          out_label = int(label.cpu().detach().numpy())\n",
    "\n",
    "          pred = inference(out_actvec, threshold, target_class_num, class_means, mr_models)\n",
    "      \n",
    "          result_preds.append(pred)\n",
    "          if is_reject:\n",
    "              result_labels.append(target_class_num)\n",
    "          else:\n",
    "              result_labels.append(out_label)\n",
    "\n",
    "    return result_preds, result_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49067991",
   "metadata": {},
   "source": [
    "* 일반 모델이 가진 분류 능력을 최대한 유지하면서 reject에 대한 판별도 높아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebb0610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.939\n",
      "Reject Accuracy:  0.525\n"
     ]
    }
   ],
   "source": [
    "# 0.35로 threshold를 정하고 탐색\n",
    "test_loader, _test_data = create_dataloader(TEST_PATH, 1, False)\n",
    "reject_loader, _reject_data = create_dataloader(REJECT_PATH, 1, False)\n",
    "target_class_num = len(os.listdir(TEST_PATH))\n",
    "\n",
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.35, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.35, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5af5eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.608\n",
      "Reject Accuracy:  0.845\n"
     ]
    }
   ],
   "source": [
    "# 0.4 threshold를 정하고 탐색\n",
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.4, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.4, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be98896",
   "metadata": {},
   "source": [
    "↑\n",
    "* 오위치 이미지에 대한 성능은 크게 증가했지만, 정위치 이미지에 대한 성능이 너무 하락함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8abd8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.888\n",
      "Reject Accuracy:  0.712\n"
     ]
    }
   ],
   "source": [
    "# 0.38 hreshold를 정하고 탐색\n",
    "test_preds, test_labels = inference_dataloader(net, test_loader, 0.38, target_class_num, class_means, mr_models)\n",
    "reject_preds, reject_labels = inference_dataloader(net, reject_loader, 0.38, target_class_num, class_means, mr_models, is_reject=True)\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, test_preds))\n",
    "print('Reject Accuracy: ', accuracy_score(reject_labels, reject_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23a2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
