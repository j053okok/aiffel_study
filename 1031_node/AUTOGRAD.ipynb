{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "* 신경망을 학습할 때 가장 자주 사용되는 알고리즘은 역전파(backpropagation)\n",
    "* 이 알고리즘에서 파라미터(모델 가중치)는 주어진 파라미터에 대한 손실 함수의 기울기에 따라 조정\n",
    "* 이러한 기울기를 계산하기 위해 파이토치에는 torch.autograd라는 미분 엔진이 내장\n",
    "* 이 엔진은 모든 계산 그래프에 대한 기울기 자동 계산을 지원"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ff94676d41fd721"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:15.937529400Z",
     "start_time": "2023-10-30T20:08:15.934699200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 텐서, 함수 및 계산 그래프\n",
    "* 이 네트워크에서 w와 b는 최적화해야 하는 파라미터\n",
    "* 따라서 이러한 변수에 대한 손실 함수의 기울기를 계산할 수 있어야 합니다. 이를 위해 해당 텐서의 requires_grad 속성을 설정\n",
    "* 역전파 함수에 대한 참조는 텐서의 grad_fn 속성에 저장"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bab502bc17f21b1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000001736A06F760>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001736A06E770>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:17.227660600Z",
     "start_time": "2023-10-30T20:08:17.224156300Z"
    }
   },
   "id": "8bd2e39c49087752"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4., 5.], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000001736A06CA60>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:04:15.428595800Z",
     "start_time": "2023-10-30T20:04:15.411003900Z"
    }
   },
   "id": "d8208755544ff08a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 덧셈 연산에 대한 기록을 `grad_fn'의 속성을 통해 확인"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d560cbcb56384cb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9., 12., 15.], grad_fn=<MulBackward0>)\n",
      "tensor(12., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * 3\n",
    "print(z)\n",
    "\n",
    "out = z.mean()\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:04:29.763672700Z",
     "start_time": "2023-10-30T20:04:29.712004300Z"
    }
   },
   "id": "908cd4b7b298cfb4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 곱셈 연산에 대한 기록 확인"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63dba76db321f60f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 그래디언트 계산하기\n",
    "* 신경망에서 파라미터의 가중치를 최적화하려면 파라미터에 대한 손실 함수의 미분을 계산\n",
    "* 이러한 도함수를 계산하기 위해 loss.backward()를 호출한 다음 w.grad 및 b.grad에서 값을 검색"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18763678e1c7721e"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0959, 0.0140, 0.3074],\n",
      "        [0.0959, 0.0140, 0.3074],\n",
      "        [0.0959, 0.0140, 0.3074],\n",
      "        [0.0959, 0.0140, 0.3074],\n",
      "        [0.0959, 0.0140, 0.3074]])\n",
      "tensor([0.0959, 0.0140, 0.3074])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:21.148994500Z",
     "start_time": "2023-10-30T20:08:21.145778900Z"
    }
   },
   "id": "ca05ba2cfba7db07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 이 노드에는 requires_grad 속성이 True로 설정되어 있습니다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d2eb4ac072ebdfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 그래디언트 추적 비활성화하기\n",
    "* 기본적으로 requires_grad가 True인 모든 텐서는 계산 이력을 추적하고 그래디언트 계산을 지원\n",
    "* 모델을 학습시킨 후 일부 입력 데이터에만 적용하려는 경우, 즉 네트워크를 통해 순방향 계산만 수행하려는 경우와 같이 그렇게 할 필요가 없는 경우도 있습니다. \n",
    "* 계산 코드를 torch.no_grad() 블록으로 둘러싸서 계산 추적을 중지할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be38ac0632b69e39"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:24.230657100Z",
     "start_time": "2023-10-30T20:08:24.224690800Z"
    }
   },
   "id": "49888e73fe35377"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 동일한 결과를 얻는 또 다른 방법은 텐서에서 detach() 메서드를 사용하는 것입니다.\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:25.034323400Z",
     "start_time": "2023-10-30T20:08:25.029419Z"
    }
   },
   "id": "bee17d4b7d6ddf1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Forward 전달에서 autograd는 두 가지 작업을 동시에 수행합니다.\n",
    "* 요청된 연산을 실행하여 결과 텐서를 계산합니다.\n",
    "* DAG에서 연산의 그래디언트 함수를 유지합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abacbf162ccf8e3c"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# 텐서를 생성하고 requires_grad를 True로 설정합니다.\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# 제곱의 합을 연산\n",
    "y = x * x\n",
    "z = y.sum()\n",
    "\n",
    "# 그라데이션 계산을 위한 backward 전달\n",
    "z.backward()\n",
    "\n",
    "# 이제 x.grad는 x에 대한 z의 기울기를 유지합니다.\n",
    "print(x.grad)  # Output: tensor([2., 4., 6.])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:02.380045100Z",
     "start_time": "2023-10-30T20:09:02.362130100Z"
    }
   },
   "id": "a9bb7c5e7c8e88ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### backward 전달에서는 DAG 루트에서 .backward()가 호출될 때 시작됩니다.\n",
    "1. autograd는 각 .grad_fn에서 그래디언트를 계산\n",
    "2. 각 텐서의 .grad 속성에 누적하고,\n",
    "3. 체인 규칙을 사용하여 잎(leaf) 텐서까지 전파합니다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "746c2f6c09f31b0f"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  4.,  32., 108.])\n"
     ]
    }
   ],
   "source": [
    "# 텐서를 생성하고 requires_grad를 True로 설정합니다.\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# 제곱의 합을 연산\n",
    "t = x * x\n",
    "y = t * t\n",
    "z = y.sum()\n",
    "\n",
    "# 그라데이션 계산을 위한 backward 전달\n",
    "z.backward()\n",
    "\n",
    "# 이제 x.grad는 x에 대한 z의 기울기를 유지합니다.\n",
    "print(x.grad)  # Output: tensor([2., 4., 6.])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:07.104777500Z",
     "start_time": "2023-10-30T20:09:07.096627300Z"
    }
   },
   "id": "9e995075e5b8107e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 텐서 그래디언트와 자코비안 곱\n",
    "* 대부분의 경우 스칼라 손실 함수가 있고 일부 매개변수에 대한 기울기를 계산해야 합니다. \n",
    "* 그러나 출력 함수가 임의의 텐서인 경우도 있습니다.\n",
    "* 이 경우 PyTorch를 사용하면 실제 그라데이션이 아닌 소위 자코비안 곱을 계산할 수 있습니다.\n",
    "* 이를 위해 다음과 같이 v 를 인수로 사용하여 backward를 호출하여 얻을 수 있습니다.\n",
    "* v의 크기는 곱을 계산하려는 원래 텐서의 크기와 같아야 합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0107faa6a4a2076"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:12:02.195287100Z",
     "start_time": "2023-10-30T20:12:02.177134800Z"
    }
   },
   "id": "44502c0367e22693"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bd32499cdbfe6b61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
